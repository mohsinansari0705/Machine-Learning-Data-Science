{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaa0352",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Fill the missing values in the numeric columns with the median using Scikit-Learn and see if that helps our best model's performance (hint: see [`sklearn.impute.SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) for more).\n",
    "2. Try putting multiple steps together (e.g. preprocessing -> modelling) with Scikit-Learn's [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) features. \n",
    "3. Try using another regression model/estimator on our preprocessed dataset and see how it goes. See the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/machine_learning_map.html) for potential model options.\n",
    "4. Try replacing the `sklearn.preprocessing.OrdinalEncoder` we used for the categorical variables with `sklearn.preprocessing.OneHotEncoder` (you may even want to do this within a pipeline) with the `sklearn.ensemble.RandomForestRegressor` model and see how it performs. Which is better for our specific dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a6a8f",
   "metadata": {},
   "source": [
    "## Example Exercise Solutions\n",
    "\n",
    "The following are examples of how to solve the above exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a69dd",
   "metadata": {},
   "source": [
    "### 1. Fill the missing values in the numeric columns with the median using Scikit-Learn and see if that helps our best model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ac7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of samples in training DataFrame: 401125\n",
      "[INFO] Number of samples in validation DataFrame: 11573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training MAE': 1950.6890126915082,\n",
       " 'Valid MAE': 5938.157777189047,\n",
       " 'Training RMSLE': 0.1017304155606623,\n",
       " 'Valid RMSLE': 0.2454972961150074,\n",
       " 'Training R^2': 0.9811165852928445,\n",
       " 'Valid R^2': 0.8820009919620663}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_log_error\n",
    "\n",
    "\n",
    "# Import train samples (making sure to parse dates and then sort by them)\n",
    "train_df = pd.read_csv(filepath_or_buffer=\"../data/bluebook-for-bulldozers/Train.csv\",\n",
    "                       parse_dates=[\"saledate\"],\n",
    "                       low_memory=False).sort_values(by=\"saledate\", ascending=True)\n",
    "\n",
    "# Import validation samples (making sure to parse dates and then sort by them)\n",
    "valid_df = pd.read_csv(filepath_or_buffer=\"../data/bluebook-for-bulldozers/Valid.csv\",\n",
    "                       parse_dates=[\"saledate\"])\n",
    "\n",
    "# The ValidSolution.csv contains the SalePrice values for the samples in Valid.csv\n",
    "valid_solution = pd.read_csv(filepath_or_buffer=\"../data/bluebook-for-bulldozers/ValidSolution.csv\")\n",
    "\n",
    "# Map valid_solution to valid_df\n",
    "valid_df[\"SalePrice\"] = valid_df[\"SalesID\"].map(valid_solution.set_index(\"SalesID\")[\"SalePrice\"])\n",
    "\n",
    "# Make sure valid_df is sorted by saledate still\n",
    "valid_df = valid_df.sort_values(\"saledate\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "# How many samples are in each DataFrame?\n",
    "print(f\"[INFO] Number of samples in training DataFrame: {len(train_df)}\")\n",
    "print(f\"[INFO] Number of samples in validation DataFrame: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "# Make a function to add date columns\n",
    "def add_datetime_features_to_df(df, date_column=\"saledate\"):\n",
    "    # Add datetime parameters for saledate\n",
    "    df[\"saleYear\"] = df[date_column].dt.year\n",
    "    df[\"saleMonth\"] = df[date_column].dt.month\n",
    "    df[\"saleDay\"] = df[date_column].dt.day\n",
    "    df[\"saleDayofweek\"] = df[date_column].dt.dayofweek\n",
    "    df[\"saleDayofyear\"] = df[date_column].dt.dayofyear\n",
    "\n",
    "    # Drop original saledate column\n",
    "    df = df.drop(\"saledate\", axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add datetime features to DataFrames\n",
    "train_df = add_datetime_features_to_df(df=train_df)\n",
    "valid_df = add_datetime_features_to_df(df=valid_df)\n",
    "\n",
    "\n",
    "# Split training data into features and labels\n",
    "X_train = train_df.drop(\"SalePrice\", axis=1)\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "# Split validation data into features and labels\n",
    "X_valid = valid_df.drop(\"SalePrice\", axis=1)\n",
    "y_valid = valid_df[\"SalePrice\"]\n",
    "\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = [label for label, content in X_train.items() if pd.api.types.is_numeric_dtype(content)]\n",
    "categorical_features = [label for label, content in X_train.items() if not pd.api.types.is_numeric_dtype(content)]\n",
    "\n",
    "\n",
    "### Filling missing values ###\n",
    "\n",
    "# Create an ordinal encoder (turns category items into numeric representation)\n",
    "ordinal_encoder = OrdinalEncoder(categories=\"auto\",\n",
    "                                 handle_unknown=\"use_encoded_value\",\n",
    "                                 unknown_value=np.nan,\n",
    "                                 encoded_missing_value=np.nan) # treat unknown categories as np.nan (or None)\n",
    "\n",
    "# Create a simple imputer to fill missing values with median\n",
    "simple_imputer_median = SimpleImputer(missing_values=np.nan,\n",
    "                                      strategy=\"median\")\n",
    "\n",
    "# Fit and transform the categorical and numerical columns of X_train\n",
    "X_train_preprocessed = X_train.copy() # make copies of the oringal DataFrames so we can keep the original values in tact and view them later\n",
    "X_train_preprocessed[categorical_features] = ordinal_encoder.fit_transform(X_train_preprocessed[categorical_features].astype(str)) # OrdinalEncoder expects all values as the same type (e.g. string or numeric only)\n",
    "X_train_preprocessed[numerical_features] = simple_imputer_median.fit_transform(X_train_preprocessed[numerical_features])\n",
    "\n",
    "# Transform the categorical and numerical columns of X_valid \n",
    "X_valid_preprocessed = X_valid.copy()\n",
    "X_valid_preprocessed[categorical_features] = ordinal_encoder.transform(X_valid_preprocessed[categorical_features].astype(str)) # only use `transform` on the validation data\n",
    "X_valid_preprocessed[numerical_features] = simple_imputer_median.transform(X_valid_preprocessed[numerical_features])\n",
    "\n",
    "\n",
    "# Create function to evaluate our model\n",
    "def show_scores(model, \n",
    "                train_features=X_train_preprocessed,\n",
    "                train_labels=y_train,\n",
    "                valid_features=X_valid_preprocessed,\n",
    "                valid_labels=y_valid):\n",
    "    \n",
    "    # Make predictions on train and validation features\n",
    "    train_preds = model.predict(X=train_features)\n",
    "    val_preds = model.predict(X=valid_features)\n",
    "\n",
    "    # Create a scores dictionary of different evaluation metrics\n",
    "    scores = {\"Training MAE\": mean_absolute_error(y_true=train_labels, y_pred=train_preds),\n",
    "              \"Valid MAE\": mean_absolute_error(y_true=valid_labels, y_pred=val_preds),\n",
    "              \"Training RMSLE\": root_mean_squared_log_error(y_true=train_labels, y_pred=train_preds),\n",
    "              \"Valid RMSLE\": root_mean_squared_log_error(y_true=valid_labels, y_pred=val_preds),\n",
    "              \"Training R^2\": model.score(X=train_features, y=train_labels),\n",
    "              \"Valid R^2\": model.score(X=valid_features, y=valid_labels)}\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# Instantiate a model with best hyperparameters \n",
    "ideal_model_2 = RandomForestRegressor(n_estimators=90,\n",
    "                                      max_depth=None,\n",
    "                                      min_samples_leaf=1,\n",
    "                                      min_samples_split=5,\n",
    "                                      max_features=0.5,\n",
    "                                      n_jobs=-1,\n",
    "                                      max_samples=None)\n",
    "\n",
    "# Fit a model to the preprocessed data\n",
    "ideal_model_2.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "\n",
    "# Evalute the model\n",
    "ideal_model_2_scores = show_scores(model=ideal_model_2)\n",
    "ideal_model_2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8db468",
   "metadata": {},
   "source": [
    "Looks like filling the missing numeric values made our `ideal_model_2` perform slightly worse than our original `ideal_model`.\n",
    "\n",
    "`ideal_model_2` had a validation RMSLE of `0.2454972961150074` where as `ideal_model` had a validation RMSLE of `0.2442769808502894`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079bda7",
   "metadata": {},
   "source": [
    "### 2. Try putting multiple steps together (e.g. preprocessing -> modelling) with Scikit-Learn's `sklearn.pipeline.Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07b0fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Scores:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training MAE': 1950.4811108609263,\n",
       " 'Valid MAE': 5949.404248941762,\n",
       " 'Training RMSLE': 0.1018847331610539,\n",
       " 'Valid RMSLE': 0.2466149973069877,\n",
       " 'Training R^2': 0.9811144473380727,\n",
       " 'Valid R^2': 0.8814994739990883}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_log_error\n",
    "\n",
    "\n",
    "# Import and prepare data\n",
    "train_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Train.csv\",\n",
    "                       parse_dates=[\"saledate\"],\n",
    "                       low_memory=False).sort_values(by=\"saledate\", ascending=True)\n",
    "\n",
    "valid_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Valid.csv\",\n",
    "                       parse_dates=[\"saledate\"])\n",
    "\n",
    "valid_solution = pd.read_csv(\"../data/bluebook-for-bulldozers/ValidSolution.csv\")\n",
    "valid_df[\"SalePrice\"] = valid_df[\"SalesID\"].map(valid_solution.set_index(\"SalesID\")[\"SalePrice\"])\n",
    "valid_df = valid_df.sort_values(\"saledate\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Add datetime features\n",
    "def add_datetime_features_to_df(df, date_column=\"saledate\"):\n",
    "    df = df.copy()\n",
    "    df[\"saleYear\"] = df[date_column].dt.year\n",
    "    df[\"saleMonth\"] = df[date_column].dt.month\n",
    "    df[\"saleDay\"] = df[date_column].dt.day\n",
    "    df[\"saleDayofweek\"] = df[date_column].dt.dayofweek\n",
    "    df[\"saleDayofyear\"] = df[date_column].dt.dayofyear\n",
    "\n",
    "    return df.drop(date_column, axis=1)\n",
    "\n",
    "# Apply datetime features\n",
    "train_df = add_datetime_features_to_df(train_df)\n",
    "valid_df = add_datetime_features_to_df(valid_df)\n",
    "\n",
    "\n",
    "# Split data into features and labels\n",
    "X_train = train_df.drop(\"SalePrice\", axis=1)\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "X_valid = valid_df.drop(\"SalePrice\", axis=1)\n",
    "y_valid = valid_df[\"SalePrice\"]\n",
    "\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = [label for label, content in X_train.items() if pd.api.types.is_numeric_dtype(content)]\n",
    "categorical_features = [label for label, content in X_train.items() if not pd.api.types.is_numeric_dtype(content)]\n",
    "\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('string_converter', FunctionTransformer(lambda x: x.astype(str))), # convert values to string\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                              unknown_value=np.nan,\n",
    "                              encoded_missing_value=np.nan)),\n",
    "])\n",
    "\n",
    "# Create preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical_transforms', numeric_transformer, numeric_features),\n",
    "        ('categorical_transforms', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create full pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=90,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=5,\n",
    "        max_features=0.5,\n",
    "        n_jobs=-1,\n",
    "        max_samples=None\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Function to evaluate the pipeline\n",
    "def evaluate_pipeline(pipeline, X_train, y_train, X_valid, y_valid):\n",
    "    # Make predictions\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    valid_preds = pipeline.predict(X_valid)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = {\n",
    "        \"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
    "        \"Valid MAE\": mean_absolute_error(y_valid, valid_preds),\n",
    "        \"Training RMSLE\": root_mean_squared_log_error(y_train, train_preds),\n",
    "        \"Valid RMSLE\": root_mean_squared_log_error(y_valid, valid_preds),\n",
    "        \"Training R^2\": pipeline.score(X_train, y_train),\n",
    "        \"Valid R^2\": pipeline.score(X_valid, y_valid)\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Fit and evaluate pipeline\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "pipeline_scores = evaluate_pipeline(model_pipeline, X_train, y_train, X_valid, y_valid)\n",
    "print(\"\\nPipeline Scores:\")\n",
    "pipeline_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32701d",
   "metadata": {},
   "source": [
    "### 3. Try using another regression model/estimator on our preprocessed dataset and see how it goes\n",
    "\n",
    "Going to use [`sklearn.linear_model.HistGradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#histgradientboostingregressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85523e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fitting HistGradientBoostingRegressor model with pipeline...\n",
      "[INFO] Evaluating HistGradientBoostingRegressor model with pipeline...\n",
      "\n",
      "Pipeline HistGradientBoostingRegressor Scores:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training MAE': 5627.763561175847,\n",
       " 'Valid MAE': 7241.016939519321,\n",
       " 'Training RMSLE': 0.2690753757199006,\n",
       " 'Valid RMSLE': 0.3026875195607664,\n",
       " 'Training R^2': 0.8653678318987129,\n",
       " 'Valid R^2': 0.8336503961646513}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_log_error\n",
    "\n",
    "\n",
    "# Import and prepare data\n",
    "train_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Train.csv\",\n",
    "                       parse_dates=[\"saledate\"],\n",
    "                       low_memory=False).sort_values(by=\"saledate\", ascending=True)\n",
    "\n",
    "valid_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Valid.csv\",\n",
    "                       parse_dates=[\"saledate\"])\n",
    "\n",
    "valid_solution = pd.read_csv(\"../data/bluebook-for-bulldozers/ValidSolution.csv\")\n",
    "valid_df[\"SalePrice\"] = valid_df[\"SalesID\"].map(valid_solution.set_index(\"SalesID\")[\"SalePrice\"])\n",
    "valid_df = valid_df.sort_values(\"saledate\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Add datetime features\n",
    "def add_datetime_features_to_df(df, date_column=\"saledate\"):\n",
    "    df = df.copy()\n",
    "    df[\"saleYear\"] = df[date_column].dt.year\n",
    "    df[\"saleMonth\"] = df[date_column].dt.month\n",
    "    df[\"saleDay\"] = df[date_column].dt.day\n",
    "    df[\"saleDayofweek\"] = df[date_column].dt.dayofweek\n",
    "    df[\"saleDayofyear\"] = df[date_column].dt.dayofyear\n",
    "\n",
    "    return df.drop(date_column, axis=1)\n",
    "\n",
    "# Apply datetime features\n",
    "train_df = add_datetime_features_to_df(train_df)\n",
    "valid_df = add_datetime_features_to_df(valid_df)\n",
    "\n",
    "\n",
    "# Split data into features and labels\n",
    "X_train = train_df.drop(\"SalePrice\", axis=1)\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "X_valid = valid_df.drop(\"SalePrice\", axis=1)\n",
    "y_valid = valid_df[\"SalePrice\"]\n",
    "\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = [label for label, content in X_train.items() if pd.api.types.is_numeric_dtype(content)]\n",
    "categorical_features = [label for label, content in X_train.items() if not pd.api.types.is_numeric_dtype(content)]\n",
    "\n",
    "\n",
    "# Create preprocessing steps for different types of values\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('string_converter', FunctionTransformer(lambda x: x.astype(str))), # convert values to string\n",
    "    ('ordinal', OrdinalEncoder(categories='auto',\n",
    "                               handle_unknown='use_encoded_value',\n",
    "                               unknown_value=np.nan,\n",
    "                               encoded_missing_value=np.nan)), \n",
    "])\n",
    "\n",
    "# Create preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical_transforms', numeric_transformer, numeric_features),\n",
    "        ('categorical_transforms', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create full pipeline\n",
    "model_pipeline_hist_gradient_boosting_regressor = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', HistGradientBoostingRegressor()) # Change model to HistGradientBoostingRegressor\n",
    "])\n",
    "\n",
    "\n",
    "# Function to evaluate the pipeline\n",
    "def evaluate_pipeline(pipeline, X_train, y_train, X_valid, y_valid):\n",
    "    # Make predictions\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    valid_preds = pipeline.predict(X_valid)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = {\n",
    "        \"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
    "        \"Valid MAE\": mean_absolute_error(y_valid, valid_preds),\n",
    "        \"Training RMSLE\": root_mean_squared_log_error(y_train, train_preds),\n",
    "        \"Valid RMSLE\": root_mean_squared_log_error(y_valid, valid_preds),\n",
    "        \"Training R^2\": pipeline.score(X_train, y_train),\n",
    "        \"Valid R^2\": pipeline.score(X_valid, y_valid)\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Fit and evaluate pipeline\n",
    "print(\"[INFO] Fitting HistGradientBoostingRegressor model with pipeline...\")\n",
    "model_pipeline_hist_gradient_boosting_regressor.fit(X_train, y_train)\n",
    "\n",
    "print(\"[INFO] Evaluating HistGradientBoostingRegressor model with pipeline...\")\n",
    "pipeline_hist_scores = evaluate_pipeline(model_pipeline_hist_gradient_boosting_regressor, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "print(\"\\nPipeline HistGradientBoostingRegressor Scores:\")\n",
    "pipeline_hist_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81928f",
   "metadata": {},
   "source": [
    "### 4. Try replacing the `sklearn.preprocessing.OrdinalEncoder` we used for the categorical variables with `sklearn.preprocessing.OneHotEncoder`\n",
    "\n",
    "> **Note:** This may take quite a long time depending on your machine. For example, on my ASUS Vivobook X1605ZA it took ~15 minutes with `n_estimators=10` (9x lower than what we used for our `best_model`). This is because using `sklearn.preprocessing.OneHotEncoder` adds many more features to our dataset (each feature gets turned into an array of 0's and 1's for each unique value). And the more features, the longer it takes to compute and find patterns between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c0b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fitting model with one hot encoded values...\n",
      "[INFO] Evaluating model with one hot encoded values...\n",
      "[INFO] Pipeline with one hot encoding scores:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Training MAE': 2118.909424271624,\n",
       " 'Valid MAE': 6161.482251483096,\n",
       " 'Training RMSLE': 0.10960067141291492,\n",
       " 'Valid RMSLE': 0.2532635777630417,\n",
       " 'Training R^2': 0.9763890510261058,\n",
       " 'Valid R^2': 0.8706567646072217}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_log_error\n",
    "\n",
    "\n",
    "# Import and prepare data\n",
    "train_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Train.csv\",\n",
    "                       parse_dates=[\"saledate\"],\n",
    "                       low_memory=False).sort_values(by=\"saledate\", ascending=True)\n",
    "\n",
    "valid_df = pd.read_csv(\"../data/bluebook-for-bulldozers/Valid.csv\",\n",
    "                       parse_dates=[\"saledate\"])\n",
    "\n",
    "valid_solution = pd.read_csv(\"../data/bluebook-for-bulldozers/ValidSolution.csv\")\n",
    "valid_df[\"SalePrice\"] = valid_df[\"SalesID\"].map(valid_solution.set_index(\"SalesID\")[\"SalePrice\"])\n",
    "valid_df = valid_df.sort_values(\"saledate\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Add datetime features\n",
    "def add_datetime_features_to_df(df, date_column=\"saledate\"):\n",
    "    df = df.copy()\n",
    "    df[\"saleYear\"] = df[date_column].dt.year\n",
    "    df[\"saleMonth\"] = df[date_column].dt.month\n",
    "    df[\"saleDay\"] = df[date_column].dt.day\n",
    "    df[\"saleDayofweek\"] = df[date_column].dt.dayofweek\n",
    "    df[\"saleDayofyear\"] = df[date_column].dt.dayofyear\n",
    "\n",
    "    return df.drop(date_column, axis=1)\n",
    "\n",
    "# Apply datetime features\n",
    "train_df = add_datetime_features_to_df(train_df)\n",
    "valid_df = add_datetime_features_to_df(valid_df)\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train = train_df.drop(\"SalePrice\", axis=1)\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "X_valid = valid_df.drop(\"SalePrice\", axis=1)\n",
    "y_valid = valid_df[\"SalePrice\"]\n",
    "\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = [label for label, content in X_train.items() if pd.api.types.is_numeric_dtype(content)]\n",
    "categorical_features = [label for label, content in X_train.items() if not pd.api.types.is_numeric_dtype(content)]\n",
    "\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('string_converter', FunctionTransformer(lambda x: x.astype(str))),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # fill missing values with the term \"missing\"\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True)) # use OneHotEncoder instead of OrdinalEncoder\n",
    "])\n",
    "\n",
    "\n",
    "# Create preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False  # Simplify feature names\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "model_one_hot_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=10,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=5,\n",
    "        max_features=0.5,\n",
    "        n_jobs=-1,\n",
    "        max_samples=None\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Function to evaluate the pipeline\n",
    "def evaluate_pipeline(pipeline, X_train, y_train, X_valid, y_valid):\n",
    "    # Make predictions\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    valid_preds = pipeline.predict(X_valid)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = {\n",
    "        \"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
    "        \"Valid MAE\": mean_absolute_error(y_valid, valid_preds),\n",
    "        \"Training RMSLE\": root_mean_squared_log_error(y_train, train_preds),\n",
    "        \"Valid RMSLE\": root_mean_squared_log_error(y_valid, valid_preds),\n",
    "        \"Training R^2\": pipeline.score(X_train, y_train),\n",
    "        \"Valid R^2\": pipeline.score(X_valid, y_valid)\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Fit and evaluate pipeline\n",
    "print(\"[INFO] Fitting model with one hot encoded values...\")\n",
    "model_one_hot_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"[INFO] Evaluating model with one hot encoded values...\")\n",
    "pipeline_one_hot_scores = evaluate_pipeline(model_one_hot_pipeline, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "print(\"[INFO] Pipeline with one hot encoding scores:\")\n",
    "pipeline_one_hot_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
